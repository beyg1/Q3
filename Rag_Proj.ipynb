{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJdfkBZfSVm4wPYOfeNyXF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/beyg1/Q3/blob/main/Rag_Proj.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9SKiQR1ikNii",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "5a32d399-f625-4836-e08f-f9ab9df03a15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.5/41.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/411.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.3/427.3 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -qU langchain-pinecone langchain-google-genai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "pinecone_api = userdata.get('pinecone_api')\n",
        "pc = Pinecone(api_key=pinecone_api)"
      ],
      "metadata": {
        "id": "cXaVJCDAnFVx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "                     # Initialization Pinecone Index\n",
        "import time\n",
        "index_name = \"test-index\"\n",
        "\n",
        "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
        "\n",
        "if index_name not in existing_indexes:\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=768,      #dimension for google's embeddings model\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
        "    )\n",
        "    while not pc.describe_index(index_name).status[\"ready\"]:\n",
        "        time.sleep(1)\n",
        "\n",
        "index = pc.Index(index_name)"
      ],
      "metadata": {
        "id": "yVyy_MoAny4G"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "                  # Initializing Embeddigs\n",
        "%pip install --upgrade --quiet  langchain-google-genai\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "import os\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "vector = embeddings.embed_query(\"hello, world!\")\n",
        "vector[:5]\n",
        "                  # Initializing Vector Store\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "vector_store = PineconeVectorStore(index=index, embedding=embeddings)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Qy1AR9--jgcE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "                     # Passing Data to Pinecone\n",
        "from uuid import uuid4\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "document_1 = Document(\n",
        "    page_content=\"I had chocalate chip pancakes and scrambled eggs for breakfast this morning.\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_2 = Document(\n",
        "    page_content=\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\",\n",
        "    metadata={\"source\": \"news\"},\n",
        ")\n",
        "\n",
        "document_3 = Document(\n",
        "    page_content=\"Building an exciting new project with LangChain - come check it out!\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_4 = Document(\n",
        "    page_content=\"Robbers broke into the city bank and stole $1 million in cash.\",\n",
        "    metadata={\"source\": \"news\"},\n",
        ")\n",
        "\n",
        "document_5 = Document(\n",
        "    page_content=\"Wow! That was an amazing movie. I can't wait to see it again.\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_6 = Document(\n",
        "    page_content=\"Is the new iPhone worth the price? Read this review to find out.\",\n",
        "    metadata={\"source\": \"website\"},\n",
        ")\n",
        "\n",
        "document_7 = Document(\n",
        "    page_content=\"The top 10 soccer players in the world right now.\",\n",
        "    metadata={\"source\": \"website\"},\n",
        ")\n",
        "\n",
        "document_8 = Document(\n",
        "    page_content=\"LangGraph is the best framework for building stateful, agentic applications!\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_9 = Document(\n",
        "    page_content=\"The stock market is down 500 points today due to fears of a recession.\",\n",
        "    metadata={\"source\": \"news\"},\n",
        ")\n",
        "\n",
        "document_10 = Document(\n",
        "    page_content=\"I have a bad feeling I am going to get deleted :(\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "documents = [\n",
        "    document_1,\n",
        "    document_2,\n",
        "    document_3,\n",
        "    document_4,\n",
        "    document_5,\n",
        "    document_6,\n",
        "    document_7,\n",
        "    document_8,\n",
        "    document_9,\n",
        "    document_10,\n",
        "]\n",
        "uuids = [str(uuid4()) for _ in range(len(documents))]\n",
        "\n",
        "vector_store.add_documents(documents=documents, ids=uuids)"
      ],
      "metadata": {
        "id": "uP3B7wHHl9Rw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "364c4564-a719-4b9a-b524-11f5a39c21f5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['74b940c2-d212-41ad-b443-1461cf8a83fc',\n",
              " '6c955c00-f49c-4d78-9837-15e7b57d6151',\n",
              " '3860d1e9-6d77-4b79-87f5-db76eb332af0',\n",
              " 'fafd6069-1c79-45c2-9b44-38a4652c5459',\n",
              " 'e2f1821f-a915-49ad-85c4-5e894960dd05',\n",
              " '396214a1-b722-4537-afe3-1219a8fe7f6f',\n",
              " '0b949e7b-93a4-48e9-b744-70bbadb2d133',\n",
              " '098b1498-a4fe-4ba0-a4a4-855f30b9a11e',\n",
              " 'a5657bc9-bdf3-4469-8aa6-1b6ad01a5eab',\n",
              " '0ac7ae6a-18c9-4d27-ad89-b1b2fb1390ef']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "                       # Data Retrieval\n",
        "                       #Direct Querry\n",
        "results = vector_store.similarity_search(\n",
        "    \"LangChain provides abstractions to make working with LLMs easy\",\n",
        "    k=2,\n",
        "    filter={\"source\": \"tweet\"},\n",
        ")\n",
        "for res in results:\n",
        "    print(f\"* {res.page_content} [{res.metadata}]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeE5SRl_vNgJ",
        "outputId": "148a8395-0338-4da1-a37d-024d6363937f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* LangGraph is the best framework for building stateful, agentic applications! [{'source': 'tweet'}]\n",
            "* LangGraph is the best framework for building stateful, agentic applications! [{'source': 'tweet'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Similarity Search\n",
        "results = vector_store.similarity_search_with_score(\n",
        "    \"Will it be hot tomorrow?\", k=1, filter={\"source\": \"news\"}\n",
        "    )\n",
        "for res, score in results:\n",
        "    print(f\"* [SIM={score:3f}] {res.page_content} [{res.metadata}]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlsBMMB7xHv4",
        "outputId": "e038908f-9652-4ef4-d443-964332e43e47"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* [SIM=0.668031] The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees. [{'source': 'news'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieval Querry by transforming Vec Store into retriever\n",
        "retriever = vector_store.as_retriever(\n",
        "    search_type=\"similarity_score_threshold\",\n",
        "    search_kwargs={\"k\": 1, \"score_threshold\": 0.5},\n",
        ")\n",
        "retriever.invoke(\"Stealing from the bank is a crime\", filter={\"source\": \"news\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyb-3Posx3cP",
        "outputId": "ade8d981-1755-47da-ad0d-2aa0e9044028"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id='d6223f96-8881-49e5-9e8c-77ce930d4466', metadata={'source': 'news'}, page_content='Robbers broke into the city bank and stole $1 million in cash.')]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Up until now retrieval has been happening after data is embedded into vectors.\n",
        "Now we will pass the relevant data that has been retrieved into the llm to generate content on the base of retrieved context."
      ],
      "metadata": {
        "id": "pPkdJPk4VncQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting Gemini's LLM model\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash-exp\",\n",
        "    temperature=0.5,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        "    # other params...\n",
        ")"
      ],
      "metadata": {
        "id": "i7kppk11Xjkg"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Making a function that will take user's querry and retrieve similar content\n",
        "#from vector db and pass it to Gem LLM so that it could generate content based\n",
        "#on specific context\n",
        "def llm_answer (querry:str):\n",
        "  #Vector Search to search for the relevant context based on user querry\n",
        "  results = vector_store.similarity_search(querry, k=1,)\n",
        "  #Transfering relevant vectors & user's querry to LLM\n",
        "  response_llm = llm.invoke(f\"Answer user's querry: {querry}, based on the context: {results}\")\n",
        "  return response_llm"
      ],
      "metadata": {
        "id": "yiz-f6m-WQRw"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm_answer(\"Analyse the security situation and weather as i am planning a trip.\")"
      ],
      "metadata": {
        "id": "U9alX6zNatJl"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "TZumu8wycyfe",
        "outputId": "58ae5663-176d-4fcc-c25a-1e240ae10b08"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Okay, based on the information provided, here's an analysis for your trip planning:\\n\\n**Security Situation:**\\n\\n*   **No information:** The provided document only contains weather information. There is **no mention of the security situation** in the context. Therefore, I cannot provide any analysis on security. You will need to consult other sources for that information, such as travel advisories, local news, or government websites.\\n\\n**Weather:**\\n\\n*   **Cloudy and Overcast:** The weather forecast for tomorrow is described as cloudy and overcast.\\n*   **Temperature:** The high temperature is expected to be 62 degrees.\\n\\n**Summary and Recommendations:**\\n\\n*   **Weather:** Be prepared for a day with cloud cover. The temperature of 62 degrees is relatively mild, so you might need a light jacket or sweater depending on your comfort level.\\n*   **Security:** **You must find information about the security situation from other sources.** Do not rely solely on this information for your travel plans. Check for travel advisories, local news, and government websites for up-to-date security information for your destination.\\n\\n**To help me provide a better analysis, please provide:**\\n\\n*   **Your destination:** Knowing where you are planning to travel is crucial for both security and weather analysis.\\n*   **Timeframe:** If your trip is not tomorrow, the weather information is not applicable.\\n*   **Any other relevant information:** Any additional information you can provide will help me give a more tailored and helpful response.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    }
  ]
}